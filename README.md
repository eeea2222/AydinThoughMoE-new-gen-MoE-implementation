# AydinThoughMoE-new-gen-MoE-implementation
AydinThough-new-gen-MoE-implementation (not tested first version)
References / Credits:

  SwiGLU Activation: Shazeer, N. (2020). "GLU Variants Improve Transformer".
  Router Z-Loss: Zoph et al. (2022). "ST-MoE: Designing Stable and Transferable Sparse Expert Models".
  Architecture: Implemented from scratch using PyTorch based on AydinThought principles.
